In this chapter, the method used to investigate the problem statement is
presented. First, the choice of method is described, motivating the dataset and
technologies used. Following this the problems used for benchmarking are
presented, including a more in-depth description of the dataset. After these
motivations, the actual implementation details are presented. Finally the
evaluations done are described, providing the parameters used for the tests
conducted.

\section{Choice of method}\label{sec:choiceofmethod}
This section will motivate the methods' three primary questions:
\begin{enumerate}
\item What databases are evaluated?
\item What dataset is used to evaluate the databases?
\item How are the databases evaluated?
\end{enumerate}

The following three sections will answer these questions, motivating
choice of databases, dataset and implementation.

\subsection{Choice of databases}\label{sec:choiceofdatabases}
The two databases chosen to evaluated are PostgreSQL and MariaDB.\@ The choice was
made based on the the fact that they are:
\begin{enumerate}
\item Modern databases with widespread use and active development;
\item Open-source projects allowing anyone to read and modify the source code;
\item And they implement state-of-the-art algorithms and methods.
\end{enumerate}

In addition to this they both cover two common use-cases: academia and
enterprises. All research papers mentioned in Chapter~\ref{chap:relatedwork} that have
implemented new algorithms or modified old ones have done so in PostgreSQL.\@
On the other hand MariaDB is compatible with MySQL, making it a common
alternative for companies to use.

An evaluation of both of these database will give a good indication of the
performance of a modern state-of-the-art query optimizer. Furthermore, as
mentioned in Section~\ref{sec:purpose} since both of them are open-source, if
one performs better than the other the code can be studied to identify areas of
improvement.

\subsection{Choice of dataset}\label{sec:dataset}
The primary focus when selecting the dataset was to use a dataset which
could capture the complexity of a real-world database and provide a realistic
challenge for the query optimizer. The primary requirements are that the dataset
feature:
\begin{enumerate}
\item Many relations with multiple indexes each;
\item\label{item:dataset:req2} More than just trivial indexes on a single row;
\item Skewed and non-uniform data for which the cardinality is not trivially
  estimated;
\item And a sufficiently large amount of data such that the database must estimate
  cardinality.
\end{enumerate}

The datasets most commonly used for evaluation of database implementations are
TPC-H~\cite{tpc_th}, TPC-DS~\cite{tpc_tha} and more recently
JOB~\cite{leis_2015_how_hgaqor}. However, none of these datasets meet
requirement~\ref{item:dataset:req2} making the problem of selecting access
methods for these databases trivial.

Instead, the dataset chosen was one taken from the real world: the dataset for
TriOptima's product triReduce~\cite{portfolio_pcsfoddt|t} which fulfill all the
requirement. The metrics of the dataset are presented in more detail in
Section~\ref{sec:benchmark}.

Another important aspect of the dataset is the queries used for evaluation.
Selecting these was done based on the following critera:
\begin{itemize}
\item The relations involved in the query must be sufficiently large as to require the
  cardinality to be estimated via sampling;
\item The data most be accessible via one or more index so that the actual index
  selection is not trivial for the query optimizer.
\end{itemize}

The two criteria are not fulfilled for more than a few relations in a database,
reducing the amount of queries relevant for evaluation. However, the queries
that do fulfill the above requirements are also those that are most interesting
to study as they are the ones that will have the longest execution time.

\subsection{Choice of implementation}\label{sec:implchoice}
The focus when implementing the tool used to evaluate the databases was to find
a tool that would allow a high-level description of the data transformations
necessary. Additionally the language must be sufficiently stable and be able to
handle potentially large amounts of data.

The language chosen that fulfill these requirements is Clojure. Clojure compiles
to bytecode that runs on the JVM, which is stable and well-used. Additionally
the language is well-suited to describing data transformations as it provides
many high-level functions for doing so.

More information regarding Clojure, the tool developed and how the data is
transformed in practice is presented in Section~\ref{sec:implementation}

\section{Benchmark problems}\label{sec:benchmark}
This section describes the problems used for benchmarking, starting with
specification of the hardware that the tests were ran on. Following this is
a description of the metrics of the dataset used. Finally the metrics for the
queries used for the evaluation are presented.

\subsection{Hardware specs}
All evaluations were done on a dedicated computer running only the databases.
The most important part of the hardware is to ensure that there is sufficient
data for both the databases and the results of the tests. For all evaluations
three hard drives were used, one for each database and one for the tool itself.

The exact specifications are:
\begin{itemize}
\item 2 \textit{Intel\,\textregistered{} Xeon\,\textregistered{} Processor
    E5\-2643 (10M Cache, 3.30 GHz, 8.00 GT/s Intel\,\textregistered{} QPI)},
  featuring 4 cores each;
\item 1 \textit{Seagate Savvio 15K.3 ST9146853SS 146GB 15000 RPM 64MB Cache SAS 6Gb/s
    2.5''}, used to store the project itself on;
\item 1 \textit{Seagate Constellation ES.3 ST4000NM0023 4TB 7200 RPM 128MB Cache SAS
    6Gb/s 3.5''}, used to store the PostgreSQL database on;
\item And 1 \textit{Seagate Constellation ES ST2000NM0001 2TB 7200 RPM 64MB Cache SAS 6Gb/s
    3.5''}, used to store the MariaDB database on.
\end{itemize}

As a final note it is worth pointing out that the effect of the hardware should
have none, or very little, effect on the query optimizer's plan selection.

\subsection{The dataset}
As detailed in Section~\ref{sec:dataset} the dataset should be sufficiently
complex in terms of indexes, table size and table values.
Table~\ref{table:dataset} presents the number of indexes, the number of rows and
the size in MB of the entire database and all relations involved in the
benchmarking, the names of the relations have been anonymized and are referred
to by an identifier such as \textit{mm} or \textit{book}.

\begin{table}
  \begin{center}
    \begin{tabu} {c c c c}
      \toprule
      & \#index & \#rows & size (MB) \\
      \midrule
      database total & 1130 & 305 & 1165290 \\
      mm & 6 & 64882651 & 9448 \\
      book & 6 & 51709 & 10 \\
      resamb & 3 & 40598 & 5 \\
      cmm & 2 & 17335822 & 1219 \\
      cmt & 9 & 52808814 & 12811 \\
      t & 35 & 115851469 & 92633 \\
      est & 32 & 33726190 & 19434 \\
      ct & 23 & 115751571 & 72320 \\
      mt & 9 & 21721256 & 4284 \\
      \bottomrule
    \end{tabu}
    \caption[The metrics for the dataset]{The metrics for the dataset used for
      evaluation of the databases. Both the metrics for the entire database and
      those of individual relations are shown. Note that the relation names have been
      anonymized and are only referred to by an identifier.}\label{table:dataset}
  \end{center}
\end{table}

The original dataset was stored in a MySQL database and was ported to
PostgreSQL and MariaDB.\@ MariaDB is made as an add-on to MySQL and thus required
no tooling, the data was just copied into a fresh install of MariaDB using a
\textit{mysqldump}. For PostgreSQL
\textit{py-mysql2pgsql}~\cite{philipsoutham_p} was used to create a dump of the
MySQL database with all MySQL specific data types converted to their most
similar PostgreSQL equivalents. The data was then read into a fresh install of
PostgreSQL.\@

In the copying process all indexes and relations were maintained, thus
maintaining essentially the same metrics for both of the copies as for the original.

\subsection{The queries}\label{sec:queries}
To evaluate the databases only one query was used. The reason is that, as
described in Section~\ref{sec:dataset}, there are few relations that can be
involved in the query as they must fulfill the important critera in terms of
indexes and size. As such one query covering all of the most complex relations was
constructed. This query can be considered to be the most complex query for the
dataset and subsets of it are used to simulate simpler scenarios.

The original query used for evaluation can be seen in
Figure~\ref{fig:sql:query1}, the relation names are once again anonymized. The
query is constructed so that the predicate used for filtering is an indexable
one. For the metrics of the relations involved see Figure~\ref{table:dataset}.

Variations, in the form of subsets involving fewer tables, were constructed to
present simpler scenarios for the query optimizer. These subsets can be seen in
Figure~\ref{fig:sql:query2}, Figure~\ref{fig:sql:query3} and Figure~\ref{fig:sql:query4}.

\begin{figure}[ht]
\begin{minted}[breaklines]{sql}
SELECT *
FROM ct JOIN t JOIN mt JOIN mm JOIN book JOIN cmt JOIN cmm JOIN est JOIN resamb
WHERE ct.key = :KEY
\end{minted}
  \caption[ The original query used for evaluation ]{The query used for
    evaluation, simplified and anonymized. The relations are joined on rows in
    common.}\label{fig:sql:query1}
\end{figure}

\begin{figure}[ht]
\begin{minted}[breaklines]{sql}
SELECT *
FROM ct JOIN t JOIN mt JOIN mm JOIN book
WHERE ct.key = :KEY
\end{minted}
  \caption[Query \#2, used for the second evaluation]{Query \#2, a subset of the
  original query used to provide a more simple scenario for the query
  optimizer.}\label{fig:sql:query2}
\end{figure}

\begin{figure}[ht]
\begin{minted}[breaklines]{sql}
SELECT *
FROM ct JOIN t JOIN mt JOIN mm JOIN book JOIN cmm JOIN cmt
WHERE ct.key = :KEY
\end{minted}
  \caption[Query \#3, used for the second evaluation]{Query \#3, a subset of the
  original query used to provide a more simple scenario for the query
  optimizer.}\label{fig:sql:query3}
\end{figure}

\begin{figure}[ht]
\begin{minted}[breaklines]{sql}
SELECT *
FROM ct
WHERE ct.key = :KEY
\end{minted}
  \caption[Query \#4, used for the second evaluation]{Query \#4, a subset of the
  original query and one that should be trivial for the query optimizer as it
  does no joins and only selects a single relation.}\label{fig:sql:query4}
\end{figure}


\section{Implementation}\label{sec:implementation}
This section will cover the implementation details of the tool used for
evaluation. The section starts with a general overview of the process of
evaluation, breaking it down into steps. Following this is a description of the
tool developed, showing how it is used and what technologies are used. After
this section comes three sections describing each of the three steps of evaluation.
Finally implementation details are provided for PostgreSQL and MariaDB.\@

Evaluating a database for a given query can be broken down into three primary
steps:
\begin{enumerate}
\item Repeatedly forcing the database to re-estimate the cardinalities and then
  generate query plans;
\item Parsing the query plans to find what access methods are used for all
  relations;
\item And finally analyzing the parsed plans to find the number of unique access
  methods used for each relation.
\end{enumerate}

These three steps must be executed in order, but they can be executed
independently of each other --- it is possible to only generate plans and save
them for later parsing and analysis.

\subsection{Overview of the tool}
The tool was implemented so as to be executed from the command line, providing
all the necessary parameters via flags. As mentioned in
Section~\ref{sec:implchoice} the tool was developed using Clojure and is
therefore typically ran via Leiningen, as shown in
Figure~\ref{fig:cmd:runtool1}.

\begin{figure}[ht]
  \begin{minted}[breaklines]{bash}
    lein run --steps='generate parse analyze' --query=queryid --repetitions=100 --samplesizes='10 100' --database=postgresql
  \end{minted}
  \caption[Using the tool to generate, parse and analyze a query]{An example of
    using the tool to generate, parse and analyze a query with some given
    parameters, such as the statistics targets (referred to as sample sizes here)
    to use.}\label{fig:cmd:runtool1}
\end{figure}

As the steps can be executed one at a time, the results for each are saved and
the tool allows the execution of only a specific set of steps at a time. An
example of this can be seen in Figure~\ref{fig:cmd:runtool2} where a previously
generated plan is parsed and analyzed.

\begin{figure}[ht]
  \begin{minted}[breaklines]{bash}
    lein run plans/xxx-000000000 --steps='parse analyze'
  \end{minted}
  \caption[Using the tool to parse and analyze a previously generated plan]{An
    example of how the tool can be used to parse and analyze a previously
    generated file containing query plans.}\label{fig:cmd:runtool2}
\end{figure}

The tool is open-source and can be found at~\cite{barksten_mbark_m}. The
repository has further documentation regarding project structure etc.

\subsection{Generating plans}\label{sec:generatingplans}
The task of generating query plans can be broken down in the following steps:
\begin{enumerate}
\item Set the statistics target used to estimate cardinalities;
\item Generate new statistics, and thus new cardinality estimates, for all
  relations involved in the query used for evaluation;
\item Find the query plan for each possible value of the query.
\end{enumerate}
These steps are then repeated a number of times to ensure that all query plans
are found.

The most relevant parts of the code used to generate plans can be found in
Figure~\ref{fig:clj:generating}. In the figure two functions can be seen:
\clj{generate-plans} and \clj{sample-and-query}.

The generation step is handled by \clj{generate-plans}, which is provided the
options for the evaluation. This function will then for each statistics target to use,
repeated the number of times specified, call \clj{sample-and-query}.

The \clj{sample-and-query} function will in turn perform all of the steps
outlined above; set the statistics target and generate new statistics via
\clj{resample-with!} and then find the query plan for all possible predicate
values via repeated calls to \clj{explain-query}.

Each plan found is directly saved to file for later use in parsing.

\begin{figure}[ht]
  \begin{minted}[breaklines]{clj}
(defn- sample-and-query [save-plan options]
  (resample-with! options)
  (doseq [param param-range]
    (save-plan (explain-query options param))))

(defn generate-plans [opts save-plan]
  (j/with-db-connection [db-con (opts->db-info opts)]
    (doseq [sample-size (:samplesizes opts)]
      (dotimes [i (:repetitions opts)]
       (sample-and-query save-plan
                         (assoc
                          opts
                          :sample-size sample-size
                          :connection db-con))))))
   \end{minted}
   \caption[The Clojure code to generate all query plans]{The relevant parts of the
     Clojure code used to generate the query plans. Some function definitions
     have been removed to improve readability.}
\label{fig:clj:generating}
\end{figure}

\subsection{Parsing the plans}\label{sec:parsing}
Parsing the generated plans is done by simply stripping all information but the
access methods from the query plans, after this is done the access methods are
grouped by what relation they access. The code for the parsing can be seen in
Figure~\ref{fig:clj:parsing}.

Parsing a plan is done with the function \clj{parse-plan}, which will find all
access methods with \clj{find-relation-accesses} and group these by their relation
with \clj{group-by-relation}. Finding the access methods in the query plan is
done by traversing the plan as a tree and calling \clj{save-if-relation-access}
on each value --- storing it, if it describes an access method.

\begin{figure}[ht]
\begin{minted}[breaklines]{clj}
(defn- save-if-relation-access [db-id o]
  (if (and (map? o) (contains? o db-id))
    (swap! relation-accesses conj o))
  o)

(defn- find-relation-accesses [db-id plan]
  (reset! relation-accesses [])
  (postwalk #(save-if-relation-access db-id %) plan)
  @relation-accesses)

(defn- group-by-relation [db-id accesses]
  (group-by
   #(get % db-id)
   accesses))

(defn parse-plan [db plan]
  (let [db-id (access-key db)]
    (group-by-relation db-id (find-relation-accesses db-id plan))))
   \end{minted}
   \caption[The Clojure code to parse a query]{The Clojure code used to parse
     the query plan output from the generation step.}
\label{fig:clj:parsing}
\end{figure}

Each generated plan is parsed and the new plan saved to another file. The main
purpose is to transform the data into something more easily analyzed.
Additionally the size of the query plans are reduced in size, reducing the
time taken to analyze all plans.

\subsection{Analyzing the plans}\label{sec:analyzingplans}
Analyzing the plans is done by merging all access methods found when parsing the
plans, keeping the distinct methods for each relation. The code for analysis can
be seen in Figure~\ref{fig:clj:analyzing}.

Analyzing all generated plans for a statistics target is done by the function
\clj{analyze-plans}, which is provided an identifier for the database evaluated,
a function to read the next plan and the total number of plans to read. The
analysis is done by reading the next plan and merging it with the previous one.
If the same relation is found in both the function \clj{conj-distinct} will add
only the new access methods found that are distinct from those previously found.
Only the index used is considered in terms of two plans being distinct from each
other.

\begin{figure}[ht]
\begin{minted}[breaklines]{clj}
(defn conj-distinct [f x y]
  (reduce
   (fn [coll v]
     (if (some #(= (f %) (f v)) coll)
       coll
       (conj coll v)))
   x y))

(defn analyze-plans [db next-plan plans-to-read]
  (loop [m {} plans-left plans-to-read]
    (if (zero? plans-left)
      m
      (recur
       (merge-with
        #(conj-distinct (fn [access] (get access (idx-key db)))
                        %1 %2)
        m (next-plan))
       (dec plans-left)))))
   \end{minted}
   \caption[The Clojure code to analyze a query]{The Clojure code used to
     analyze the parsed output. The code will merge the maps generated, only
     keeping the unique access methods for each relation.}
\label{fig:clj:analyzing}
\end{figure}

The analysis is done for each statistics target, and the resulting map of relation to
access methods is saved for later study.

\subsection{PostgreSQL}\label{sec:postgresql}
In PostgreSQL the SQL commands used are quite straightforward, one is used to
delete all previously gathered statistics, one to set the statistics target and
finally an \sql{ANALYZE} is called for each relation involved in the query being
evaluated. The specific commands used can be seen in
Figure~\ref{fig:sql:pganalyze}, the value of the statistics target is provided as a
host variable, as it will depend on the options used when evaluating a database
and query.

\begin{figure}[ht]
\begin{minted}[breaklines]{postgresql}
  DELETE FROM pg_statistics;
  SET default_statistics_target TO :STATISTICS_TARGET:
  ANALYZE table1;
  ANALYZE table2;
\end{minted}
\caption[Generating new cardinality estimates in PostgreSQL.]{The SQL commands
  used to first delete all statistics in PostgreSQL, set the statistics target
  and finally analyze all relations involved in the query.}
\label{fig:sql:pganalyze}
\end{figure}

\subsection{MariaDB}\label{sec:mariadb}
In MariaDB the commands used to estimate cardinality are storage engine specific,
in this case InnoDB is the storage engine used. It is worth noting that InnoDB only
provides MariaDB with statistics --- it is MariaDB that does the actual
query optimization.

It is necessary in InnoDB to ensure that the statistics used are those
generated, to do this no persistent statistics are saved. Furthermore, no data
is deleted between estimates as it is neither possible nor necessary --- the old
statistics are overwritten by the new. Finally the relations are all analyzed in
one single \sql{ANALYZE} call.

The specific commands used can be seen in Figure~\ref{fig:sql:resamplemdb}.

\begin{figure}[ht]
\begin{minted}[breaklines]{mysql}
  SET GLOBAL innodb_stats_persistent='OFF';
  SET GLOBAL innodb_stats_auto_recalc='OFF';
  SET GLOBAL innodb_stats_transient_sample_pages = :STATISTICS_TARGET:
  ANALYZE TABLE table1, table2;
\end{minted}
\caption[Generating new cardinality estimates in MariaDB.]{The SQL commands used to
first ensure that MariaDB will not use some other stats than those we gather,
then set the statistics target and finally analyze all relations involved in the query.}
\label{fig:sql:resamplemdb}
\end{figure}

\section{Evaluation}\label{sec:evaluation}
In order to evaluate the database the dataset described in~\ref{sec:dataset} and
query \#1, shown in Figure~\ref{fig:sql:query1}. The tool supports several
options to be set and the values used for the evaluation can be seen in
Table~\ref{table:evaluation1}, which shows the initial tests conducted. The
results from these tests warranted further evaluation and the options for these
tests can be seen in Table~\ref{table:evaluation2}. The same values were used
when testing both PostgreSQL and MariaDB.\@

\begin{table}
  \begin{center}
    \begin{tabu} {c c c c}
      \toprule
      query & repetitions & statistics targets \\
      \midrule
      \#1 & 50 & 1, $d$, $2d$ \\
      \#2 & 50 & 1, $d$, $2d$ \\
      \bottomrule
    \end{tabu}
    \caption[The number of repetitions and statistics targets used for the first
    evaluation]{The number of repetitions and the statistics targets, where $d$
      refers to the default statistics target for the database, used for the
      evaluation. The tests are conducted with three different statistics
      targets in order to identify the effect the cardinality estimate has on
      the access methods used.}\label{table:evaluation1}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \begin{tabu} {c c c c}
      \toprule
      database & default statistics target \\
      \midrule
      MariaDB & 20 \\
      PostgreSQL & 100 \\
      \bottomrule
    \end{tabu}
    \caption[The default statistics targets for MariaDB and PostgreSQL]{The
      default statistics targets for MariaDB and PostgreSQL, the reason they
      vary so considerably in size is because they determine different factors
      --- it is not the case that PostgreSQL has a 5 times higher default than
      MariaDB.}\label{table:defaults}
  \end{center}
\end{table}

In order to find what effect the cardinality estimate has on the access methods
used the tests are done in two different scenarios: one with a low quality
cardinality estimate and one with a quality estimate. To simulate a low quality
cardinality estimate, a low statistics target is used, causing a low sample size
to be used. The reason is that, as described in
Section~\ref{sec:collecting_statistics}, a small sample size will cause a small
fraction of the rows in the relation to be used when estimating the cardinality
--- which in turn will increase the risk of incorrect estimates.

The statistics target is not exactly the same for the two databases, for MariaDB
it is the number of pages used when sampling and for PostgreSQL it is the amount
of information stored for each relation. The effect of setting a low or high
statistics target is however the same --- the higher the value the better the
cardinality estimate.

The exact options used for the tests can be seen in
Table~\ref{table:evaluation1}, which shows what queries were tested, the
repetitions done and the statistics target used for each of the tests. Two of
the tests done use the variable $d$, which refers to the default statistics
target of the databases, the values of which can be seen in
Figure~\ref{table:defaults}. The options --- three statistics targets and
50 repetitions --- were kept small out of necessity, the tests for each database
must be run sequentially and testing one query with one statistics target takes
roughly 30 hours of execution time.

As can be seen in the table, the lowest statistics target used is 1 for both
databases. The reason is that it is done in order to simulate the worst possible
scenario for the databases --- the case where they must use the potentially most
incorrect cardinality estimate.

To contrast the worst case scenario the databases are then tested with two more
statistics targets: the default statistics target; and double the default value. The
reason is that the statistics target used is not controlled in the same way for
both databases, thus using the default value as baseline provides a way to
simulate a more similar scenario.

Finally, an additional evaluation was done in order to identify if there were
factors other than the cardinality estimate that caused the varying access
methods. These tests are therefore done with one repetition --- making all
generated query plans be based on the same cardinality estimate. The options for
the tests can be seen in Figure~\ref{table:evaluation2}.

\begin{table}
  \begin{center}
    \begin{tabu} {c c c c}
      \toprule
      queries & repetitions & statistics target \\
      \midrule
      \#1, \#3, \#4  & 1 & 1 \\
      \bottomrule
    \end{tabu}
    \caption[The number of repetitions and statistics target used for second evaluation]{The
      number of repetitions and statistics target used for the evaluation. The
      tests are done with no repetitions in order to focus on what other factors
      cause varying access methods to be used.}\label{table:evaluation2}
  \end{center}
\end{table}