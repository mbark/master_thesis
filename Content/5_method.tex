In this chapter, the methods used to investigate the problem statement are
presented. First, the choice of method is described briefly and motivated, this
section will also cover the databases used and why these were picked. Following
this section comes a section describing the benchmark problems, that is the
data set used and the corresponding queries. Finally the implementation details
of the tool used is described in more detail.

\section{Choice of method}\label{sec:choiceofmethod}
This section will cover first why the databases used for evaluation were
chosen, following this dataset used for evaluation is motivated and finally a
short description of the technologies used.

\subsection{Choice of databases}\label{sec:choiceofdatabases}
The two database chosen to be evaluated are PostgreSQL and MariaDB.
Both of these databases fulfill the following criteria:
\begin{enumerate}
\item Modern databases that see much use and development;
\item Open-source projects with code that anyone can read, modify and help develop;
\item And they implement state-of-the-art algorithms and methods.
\end{enumerate}

In addition to this PostgreSQL is the typical choice for academic evaluation.
All research papers mentioned in Chapter~\ref{chap:relatedwork} that have
implemented new algorithms or modified old ones have done so in PostgreSQL.
MariaDB on the other hand is compatible with MySQL making it a common
alternative for enterprises. The two databases should therefore cover the most
commonly used open-source databases for academia and enterprises.

Studying these two databases should cover how well a modern state-of-the-art
query optimizer performs. In addition, as mentioned in Section~\ref{sec:purpose}
since both of them are open-source, if one performs better than the other the
code can be studied to identify areas of improvement.

\subsection{Choice of dataset}
The dataset used for evaluation is one taken directly from the real-world, this
is to ensure that the database has all the traits of a real database:
non-trivial indexes, non-uniform and skewed data as well as a considerable
amount of data.

One important reason that the dataset was selected was that most of the ones
currently used for evaluation such as TPC-H, TPC-DS or more recently JOB (REFERENCE), all
have trivial index setups; that is one index on the primary key for each
relation. As such, studying how the query optimizer selects index for these is
irrelevant: there is just one index trivially found to be correct.

The relevant statistics of the dataset are shown in Section~\ref{sec:benchmark}

\subsection{Choice of technologies used}
The technologies used to develop the tool used for evaluation were chosen to be
Clojure, using a JDBC driver. The reason these were selected is that Clojure
runs on the JVM, making it platform independent. In addition most of the work
done by the tool is to process and transform data in the form of JSON, a task
that Clojure is well suited to.

\section{Benchmark problems}\label{sec:benchmark}
This section will give a description if the problems used for benchmarking the
query optimizers. The section will start with a description of the relevant
statistics of the data set, following that is a description of the queries
chosen to evaluate the query optimizer.

\subsection{Hardware used}
The hardware used for testing is a dedicated computer running only the databases
and nothing else. The configuration parameters for the database are those of a
standard setup for PostgreSQL and MariaDB respectively.

The exact specifications of the hardware are not relevant as the choice of index
selection does not depend on those, but rather the software of the database itself.

\subsection{The data set}
- number of tables, size of some tables, number of indexes etc

\subsection{The queries}
- focus on using tables with many indexes
- join several tables to improve errors and increase difficulty for the optimizer
- focus on using realistic queries actually used by trioptima to allow for
better and more realistic modeling

\section{Implementation}
This section will cover the implementation details of the tool, starting with
some of the technologies and frameworks used. Following this is a description of
the two steps of the analysis. Finally the queries used for PostgreSQL and
MariaDB respectively are presented.

The evaluation process is split into three steps:
\begin{enumerate}
\item Repeatedly executing the query with different sample sizes to generate
  query plans;
\item Parsing the query plans to find what access methods are used for all
  relations;
\item And finally analyzing the parsed plans to find the number of unique access
  methods used for each relation;
\end{enumerate}

To increase robustness and allow further analysis the results for each step are
saved and can be performed independently on the other.

The tool was implemented in Clojure, using JDBC to connect to and query the
databases. For more information about the project, refer to the documentation at
the repo (REFERERA). The typical use of the tool is to perform all steps in one
Figure~\ref{fig:cmd:runtool1}. For the case when not all steps need to done, the
tool can take a file to read from as the first input, see
Figure~\ref{fig:cmd:runtool2} for an example of how this can be done.

\begin{figure}[ht]
  \begin{minted}[breaklines]{bash}
    lein run steps='generate parse analyze' query=queryid repetitions=100 samplesizes='10 100' --database=postgresql
  \end{minted}
  \caption[Using the tool to generate, parse and analyze a query]{An example of
    using the tool to generate, parse and analyze a query with some given
    parameters, such as the sample sizes to use.}
  \label{fig:cmd:runtool1}
\end{figure}

\begin{figure}[ht]
  \begin{minted}[breaklines]{bash}
    lein run plans/xxx-000000000 steps='parse analyze'
  \end{minted}
  \caption[Using the tool to parse and analyze a previously generated plan]{An
    example of how the tool can be used to parse and analyze a previously
    generated file containing query plans.}
  \label{fig:cmd:runtool2}
\end{figure}

The following sections will describe the implementation details for each of the
three steps: generation, parsing and finally analysis.

\subsection{Generating plans}\label{sec:generatingplans}
Generating the plans is done by setting the statistics target for the database's
\texttt{ANALYZE}, sampling new data with the target and then finding the query plan for
the query. This process is then repeated a number of times for each sample size
to ensure that all possible access methods are found.

The relevant parts of the code that generate the plans can be seen in
Figure~\ref{fig:clj:generating}. Some of the functions used have been cut to
improve readability, most notably are:
\begin{itemize}
\item resample-with, which will delete old statistics gathered if necessary, set
  the statistics target and then make the database resample;
\item explain-query, which will find the query plan found for the current query,
  typically through using the \texttt{EXPLAIN} command in SQL.
\end{itemize}
For the full code refer to Appendix (REFEREA).

\begin{figure}[ht]
  \begin{minted}[breaklines]{clj}
(defn- sample-and-query [db-con db id n]
  (delay
    (resample-with! db-con db id n)
    (map
     #(explain-query db-con db id %)
     (range 1 1000))))

(defn- repeat-query [db-con db id sample-size repetitions]
  (repeatedly
   repetitions
   #(sample-and-query db-con db id sample-size)))

(defn- plans-for-query [db-con db query-id sample-sizes repetitions]
  (map #(repeat-query db-con db query-id % repetitions) sample-sizes))

(defn generate-plans [opts]
  (swap! db-connection
         (assoc :connection
                (j/get-connection ((:database opts) db-specs))))
  (plans-for-query {:connection @db-connection}
                   (:database opts)
                   (:query opts)
                   (:samplesizes opts)
                   (:repetitions opts)))
   \end{minted}
   \caption[The clojure code to generate a query]{The relevant parts of the
     clojure code used to generate the query plans. Some function definitions
     have been removed to improve readability, see the Appendix (REFERERA) for
     the full code}
\label{fig:clj:generating}
\end{figure}

\subsection{Parsing the plans}\label{sec:parsing}
Parsing the plans generated by the generate step, described
in~\ref{sec:generatingplans}, consists of identifying all the access
methods in the query plan and saving these as a list for the next step:
analysis, which is described in section~\ref{sec:analyzingplans}.

The parsing will collate the results of all repetitions per sample size
in order to find all possible access methods for those two parameters.

The source code for the parsing can be seen in Figure~\ref{fig:clj:parsing}. The
parsing step starts with the function parse-plans, which will find the relation
accesses used for each plan and then group it by the relation used.

\begin{figure}[ht]
\begin{minted}[breaklines]{clj}
(def relation-accesses (atom []))

(defn db->index-access-identifier [db]
  (cond
    (= db :postgresql) "Alias"
    (= db :mariadb) "table"))

(defn- save-if-relation-access [db o]
  (if (and (map? o) (contains? o (db->index-access-identifier db)))
    (swap! relation-accesses conj o))
  o)

(defn- find-relation-accesses [db plan]
  (reset! relation-accesses [])
  (postwalk #(save-if-relation-access db %) plan)
  @relation-accesses)

(defn- group-by-relation [db accesses]
  (apply merge-with concat
         (map
          (fn [l] (group-by #(get % (db->index-access-identifier db)) l))
          accesses)))

(defn parse-plans [db plans]
   (let [accesses (map #(find-relation-accesses db %) plans)
         by-relation (group-by-relation db accesses)]
     by-relation))
   \end{minted}
   \caption[The clojure code to parse a query]{The clojure code used to parser
     the query plan output from the generation step.}
\label{fig:clj:parsing}
\end{figure}

\subsection{Analyzing the plans}\label{sec:analyzingplans}
Analyzing the plans is done by simply removing all the duplicate access methods
for each relation. The code for this can be seen in
Figure~\ref{fig:clj:analyzing}. The analysis is done for each plan from the
parsing, that is for each sample size used.

\begin{figure}[ht]
\begin{minted}[breaklines]{clj}
(defn db->index-access-identifier [db]
  (cond
    (= db :postgresql) "Alias"
    (= db :mariadb) "table"))

(defn analyze-plan [db plan]
  (zipmap
   (keys plan)
   (map
    (fn [accesses]
      (count (distinct-by
              #(get % (db->index-access-identifier db))
              accesses)))
    (vals plan))))
   \end{minted}
   \caption[The clojure code to analyze a query]{The Clojure code used to
     analyze the parsed output. The code will simply change the map so that it
     only retains the unique access methods used for each plan.}
\label{fig:clj:analyzing}
\end{figure}

\subsection{PostgreSQL}
The SQL commands used to resample the data in PostgreSQL can be seen in
Figure~\ref{fig:sql:pganalyze}. Between every new \texttt{ANALYZE} the statistics
are deleted.

\begin{figure}[ht]
\begin{lstlisting}[language=SQL]
  DELETE FROM pg_statistics;
  SET default_statistics_target TO :SAMPLE_SIZE;
  ANALYZE table1;
  ANALYZE table2;
\end{lstlisting}
\caption[The SQL commands used to resample inPostgreSQL.]{The SQL commands used
  to first delete all statistics in PostgreSQL, set the statistics target and
  finally analyze all tables involved in the query.}
\label{fig:sql:pganalyze}
\end{figure}

\subsection{MariaDB}
The SQL commands used to resample the data in MariaDB can be seen in
Figure~\ref{fig:sql:resamplemdb}, it is worth noting that the statistics are not
deleted between each resampling here, as InnoDB does not support that functionality.

\begin{figure}[ht]
\begin{lstlisting}[language=SQL]
  SET GLOBAL innodb_stats_persistent='OFF';
  SET GLOBAL innodb_stats_auto_recalt='OFF';
  SET GLOBAL innodb_stats_transient_sample_pages = :SAMPLE_SIZE;
  ANALYZE TABLE table1, table2;
\end{lstlisting}
\caption[The SQL commands used to resample in MariaDB.]{The SQL commands used to
first ensure that MariaDB will not use some other stats than those we gather,
then set the statistics target and finally analyze all tables involved in the query.}
\label{fig:sql:resamplemdb}
\end{figure}

\section{Evaluation}
Here are some the results found from the evaluation.